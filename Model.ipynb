{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67a61b66-d49d-4fe2-a87b-1e2c310def83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=pd.errors.PerformanceWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2c4d6f5-3c87-48d3-8157-3393b675da5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def merge(orders, nodes_path):\n",
    "    nodes = pd.read_csv(nodes_path)\n",
    "    mean_speed_by_hour = pd.read_csv(\"data/mean_speed_by_hour_concat.csv\", )\n",
    "    merged = pd.merge(nodes, orders, how='left', on='Id')\n",
    "    merged = pd.merge(merged, mean_speed_by_hour, how='left', on='Id')\n",
    "    merged = merged.dropna()\n",
    "    \n",
    "    return merged\n",
    "    \n",
    "train_orders = pd.read_csv(\"data/orders.csv\")\n",
    "test_orders = pd.read_csv(\"data/final_test.csv\")\n",
    "\n",
    "train_df = merge(train_orders, \"data/nodes.csv\")\n",
    "test_df =  merge(test_orders, 'data/nodes_test.csv')\n",
    "\n",
    "df = pd.concat([train_df, test_df])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ca504e",
   "metadata": {},
   "source": [
    "#### Weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "efde3e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "wearher_df = pd.read_excel('data/weather.xlsx')\n",
    "\n",
    "cols_to_process = ['Temperature', 'Dew Point', 'Humidity', 'Wind Speed', 'Wind Gust', 'Pressure', 'Precip.']\n",
    "\n",
    "def process_numerical(x):\n",
    "    return re.compile(r'(\\d+(?:\\.\\d+)?)').findall(x)[0]\n",
    "\n",
    "for col in cols_to_process:\n",
    "    wearher_df[col] = wearher_df[col].apply(lambda x: process_numerical(x)).astype(float)\n",
    "\n",
    "    \n",
    "wearher_df['isWindy'] = wearher_df['Condition'].apply(lambda x: 1 if len(x.split('/'))==2 else 0)\n",
    "wearher_df['Condition'] = wearher_df['Condition'].apply(lambda x: x.split('/')[-1] if len(x.split('/'))==2 else x)\n",
    "\n",
    "wearher_df['time_h_m'] = pd.to_datetime(wearher_df['Time']).apply(lambda x: x.strftime(\"%H:%M\"))\n",
    "wearher_df.drop('Time', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01886554",
   "metadata": {},
   "source": [
    "#### Merge weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e769a80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def merge_weather(merged):\n",
    "    merged['time_h_m'] = pd.to_datetime(pd.to_datetime(merged['running_time'])\n",
    "                                       .apply(lambda x: x.strftime(\"%H:%M\")))\n",
    "    wearher_df['time_h_m'] = pd.to_datetime(wearher_df['time_h_m'])\n",
    "\n",
    "    merged.sort_values('time_h_m', inplace=True)\n",
    "    wearher_df.sort_values('time_h_m', inplace=True)\n",
    "\n",
    "    merged = pd.merge_asof(merged, wearher_df, on='time_h_m')\n",
    "    merged.drop('time_h_m', axis=1, inplace=True)\n",
    "\n",
    "    wind = pd.get_dummies(merged['Wind'])\n",
    "\n",
    "    merged.drop(['Wind'], axis=1, inplace=True)\n",
    "\n",
    "    condition = pd.get_dummies(merged['Condition'])\n",
    "\n",
    "    merged.drop(['Condition'], axis=1, inplace=True)\n",
    "\n",
    "    merged = pd.concat([merged, wind, condition],axis=1)\n",
    "    \n",
    "    return merged\n",
    "\n",
    "df = merge_weather(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a4c2ba1-be83-4442-b4f7-93fd658d45b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_preprocess(df):\n",
    "    df['running_time'] = pd.to_datetime(df['running_time'])\n",
    "    df['seconds'] = df['running_time'].apply(lambda x: x.hour * 3600 + x.minute * 60 + x.second)\n",
    "\n",
    "    df['mean_time'] = df['route_distance_km']/df['speed']\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "df = time_preprocess(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc7334b7-59ca-4c0f-adc8-ef210a654d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def agregate_df(df):\n",
    "    agregated_df = pd.DataFrame()\n",
    "\n",
    "    gpby = df.groupby('Id')\n",
    "\n",
    "    for name in df.columns:\n",
    "        if name == 'distance':\n",
    "            agregated_df[name] = gpby[name].sum()\n",
    "        elif name in ['node_start','node_finish','running_time','completed_time']:\n",
    "            pass\n",
    "        else:\n",
    "            agregated_df[name] = gpby[name].mean()\n",
    "    \n",
    "    agregated_df['node_list'] = gpby['node_start'].apply(list)+gpby['node_finish'].apply(list)\n",
    "            \n",
    "    for i in agregated_df.index.tolist():\n",
    "        agregated_df.at[i, 'node_list'] = list(set(agregated_df['node_list'][i]))\n",
    "    \n",
    "    try:\n",
    "        df = df.drop(['Id','node_start','node_finish','running_time','completed_time'],axis=1)\n",
    "    except:\n",
    "        df = df.drop(['Id', 'node_start','node_finish','running_time'],axis=1)\n",
    "    \n",
    "    return agregated_df\n",
    "\n",
    "df = agregate_df(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a003d8-6109-4d6e-95ea-56e5453cc229",
   "metadata": {},
   "source": [
    "# OSM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c7f72daa-2252-42dc-a1cd-40cd141cd9ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alimb\\AppData\\Local\\Temp\\ipykernel_4024\\3993110649.py:5: DtypeWarning: Columns (2,3,5,7,8,11,13,14,16,23,26,27,28,29,30,32,38,39,42,43,44,45,47,49,51,52,56,57,58,59,61,63,64,65,66,67,71,74,75,78,79,81,82) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  osm_fin = pd.read_csv('data/osm.csv',index_col=0)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "def add_osm(df):\n",
    "    osm_fin = pd.read_csv('data/osm.csv',index_col=0)\n",
    "\n",
    "    # shop replace travel_agency 2 yes\n",
    "\n",
    "    del_list = [\n",
    "    'opening_hours', 'name:en', 'mapillary','name:uk','addr:postcode','addr:city','traffic_sign:forward','motor_vehicle:conditional',\n",
    "    'name:ru','addr:housenumber','local_ref','man_made','addr:street','natural','website','source','water_source','description','cuisine','power',\n",
    "    'ref_name','phone','old_name','name:be', 'addr:state','name','ref','operator','maxheight']\n",
    "\n",
    "    osm_fin = osm_fin[[x for x in list(osm_fin) if x not in del_list]]\n",
    "\n",
    "    df_dummies = pd.get_dummies(osm_fin, columns=list(osm_fin)[1:])\n",
    "    df_dummies = df_dummies.drop(columns=[s for s in list(df_dummies) if \"_0\" in s])\n",
    "    df_dummies = df_dummies.drop(columns=[s for s in list(df_dummies) if \"_no\" in s])\n",
    "\n",
    "    for index in list(df_dummies)[1:]:\n",
    "        df[index] = 0\n",
    "\n",
    "    slice_list = []\n",
    "\n",
    "    for index in range(len(df)):\n",
    "        ans = df_dummies[df_dummies.id.isin(df.node_list.values[index])].sum().values.tolist()[1:]\n",
    "        slice_list.append(ans)\n",
    "\n",
    "    for count, index in enumerate(df.index.tolist()):\n",
    "        df.loc[index, list(df_dummies)[1:]] = slice_list[count]\n",
    "\n",
    "    df = df.drop(columns=['node_list'])\n",
    "    return df\n",
    "\n",
    "# train_df = add_osm(train_df)\n",
    "# test_df =  add_osm(test_df)\n",
    "\n",
    "df = add_osm(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "677d6a8e-5b63-41bf-ad5e-7d8722ecb1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = df[df['Id'].isin(test_df['Id'].unique())]\n",
    "train_df = df[~df['Id'].isin(test_df['Id'].unique())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "331bdaf8-cde7-4f12-a6c3-b5f2080828a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alimb\\AppData\\Local\\Temp\\ipykernel_4024\\4257643742.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_df.drop(['Id'],axis=1, inplace=True)\n",
      "C:\\Users\\alimb\\AppData\\Local\\Temp\\ipykernel_4024\\4257643742.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df.drop(['Id'],axis=1, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_df.drop(['Id'],axis=1, inplace=True)\n",
    "test_df.drop(['Id'],axis=1, inplace=True)\n",
    "\n",
    "train_df.reset_index(drop=True, inplace=True)\n",
    "test_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "y_train = np.array(train_df['delta_time']).reshape(-1, 1)\n",
    "X_train = train_df.drop(['delta_time'], axis=1)\n",
    "\n",
    "# y_test = np.array(test_df['delta_time']).reshape(-1, 1)\n",
    "X_test = test_df.drop(['delta_time'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6fb0c52c-3196-41b8-bb3c-31ec1a711012",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X_scaler = StandardScaler()\n",
    "X_train = X_scaler.fit_transform(X_train)\n",
    "\n",
    "X_test = X_scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8d1fed2f-7a75-4ed9-b2d1-3e5dd82b372c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "params = {'max_depth': 3,\n",
    "                'min_child_weight': 4,\n",
    "                'gamma': 0.15,\n",
    "                'colsample_bytree': 0.85,\n",
    "                'subsample': 1.0}\n",
    "\n",
    "kf = KFold(n_splits=10, random_state=42, shuffle=True)\n",
    "predictions_array =[]\n",
    "CV_score_array    =[]\n",
    "for train_index, test_index in kf.split(X_train):\n",
    "    X_tr, X_val = X_train[train_index], X_train[test_index]\n",
    "    y_tr, y_val = y_train[train_index], y_train[test_index]\n",
    "    regressor = XGBRegressor(**params)\n",
    "    regressor.fit(X_tr, y_tr)\n",
    "    predictions_array.append(regressor.predict(X_test))\n",
    "\n",
    "predictions = np.mean(predictions_array,axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "757f646e-894b-4525-a6e1-fdcbe35e1bc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop training because you reached max_epochs = 300 with best_epoch = 85 and best_val_0_rmse = 123.96305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alimb\\anaconda3\\envs\\py38\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop training because you reached max_epochs = 300 with best_epoch = 118 and best_val_0_rmse = 124.33405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alimb\\anaconda3\\envs\\py38\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop training because you reached max_epochs = 300 with best_epoch = 140 and best_val_0_rmse = 121.84334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alimb\\anaconda3\\envs\\py38\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop training because you reached max_epochs = 300 with best_epoch = 93 and best_val_0_rmse = 121.82551\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alimb\\anaconda3\\envs\\py38\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop training because you reached max_epochs = 300 with best_epoch = 77 and best_val_0_rmse = 127.39133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alimb\\anaconda3\\envs\\py38\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop training because you reached max_epochs = 300 with best_epoch = 136 and best_val_0_rmse = 118.4771\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alimb\\anaconda3\\envs\\py38\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop training because you reached max_epochs = 300 with best_epoch = 133 and best_val_0_rmse = 114.03483\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alimb\\anaconda3\\envs\\py38\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop training because you reached max_epochs = 300 with best_epoch = 80 and best_val_0_rmse = 125.45485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alimb\\anaconda3\\envs\\py38\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop training because you reached max_epochs = 300 with best_epoch = 144 and best_val_0_rmse = 123.33119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alimb\\anaconda3\\envs\\py38\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop training because you reached max_epochs = 300 with best_epoch = 87 and best_val_0_rmse = 125.00476\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alimb\\anaconda3\\envs\\py38\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    }
   ],
   "source": [
    "from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "\n",
    "kf = KFold(n_splits=10, random_state=42, shuffle=True)\n",
    "predictions_array =[]\n",
    "CV_score_array    =[]\n",
    "for train_index, test_index in kf.split(X_train):\n",
    "    X_tr, X_val = X_train[train_index], X_train[test_index]\n",
    "    y_tr, y_val = y_train[train_index], y_train[test_index]\n",
    "    regressor = TabNetRegressor(verbose=0,seed=42)\n",
    "    regressor.fit(X_train=X_tr, y_train=y_tr,\n",
    "              eval_set=[(X_val, y_val)],\n",
    "              patience=300, max_epochs=300,\n",
    "              eval_metric=['rmse'])\n",
    "    CV_score_array.append(regressor.best_cost)\n",
    "    predictions_array.append(regressor.predict(X_test))\n",
    "\n",
    "predictions = np.mean(predictions_array,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6964871d-c3a3-4600-a672-560fcb7581f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
